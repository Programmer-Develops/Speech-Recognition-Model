# -*- coding: utf-8 -*-
"""spech-recognition

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WJeSXSGv1bly2ey1UXFm96SFOzGNmS9e
"""

# Display waveform and spectrogram of the audio file

import librosa
import librosa.display
import matplotlib.pyplot as plt

y, sr = librosa.load('voice.mp3')
plt.figure(figsize=(10, 4))
librosa.display.waveshow(y, sr=sr)
plt.title('Waveform')
plt.show()

# Spectrogram
X = librosa.stft(y)
Xdb = librosa.amplitude_to_db(abs(X))
plt.figure(figsize=(10, 4))
librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz')
plt.colorbar()
plt.title('Spectrogram')
plt.show()

# Compute Mel Spectrogram

import librosa
import librosa.display
import matplotlib.pyplot as plt
import numpy as np

# Load audio
y, sr = librosa.load('voice.mp3')

# Compute Mel Spectrogram
S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=8000)

# Convert to Decibel scale
S_dB = librosa.power_to_db(S, ref=np.max)

# Plot it
plt.figure(figsize=(12, 4))
librosa.display.specshow(S_dB, sr=sr, x_axis='time', y_axis='mel', fmax=8000)
plt.colorbar(format="%+2.0f dB")
plt.title('Mel Spectrogram (dB)')
plt.tight_layout()
plt.show()

import librosa
import librosa.display
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.nn.functional as F

# Load audio
y, sr = librosa.load('voice.mp3')

# Extract MFCC features
mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
mfccs = torch.tensor(mfccs.T, dtype=torch.float32).unsqueeze(0)  # [1, time, features]

labels = ["the", "of", "is", "play", "my", "but", "bet"]

# Traditional RNN model
class TraditionalRNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.rnn = nn.GRU(input_size=13, hidden_size=32, batch_first=True)
        self.fc = nn.Linear(32, len(labels))  # match the number of labels

    def forward(self, x):
        out, _ = self.rnn(x)
        return self.fc(out[:, -1, :])  # Use last time step

# Run model
model = TraditionalRNN()
output = model(mfccs)

# Convert logits to probabilities
probs = F.softmax(output, dim=1).detach().numpy().flatten()

# Find predicted label
predicted_idx = np.argmax(probs)
predicted_label = labels[predicted_idx]

# Print readable output
print("Probabilities:")
for i, label in enumerate(labels):
    print(f"{label:>10}: {probs[i]:.4f}")

print(f"\n Predicted label: **{predicted_label.upper()}**")

# Whisper: End-to-end speech recognition

from transformers import pipeline

# Load Whisper model
asr = pipeline("automatic-speech-recognition", model="openai/whisper-small")

# Run inference on audio
result = asr("voice.mp3")

# Print the transcription
print("Whisper Transcription:", result['text'])

import torch
import librosa
import numpy as np
import matplotlib.pyplot as plt
from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor

# STEP 1: Load the pretrained model + processor
model_name = "facebook/wav2vec2-base-960h"
processor = Wav2Vec2Processor.from_pretrained(model_name)
model = Wav2Vec2ForCTC.from_pretrained(model_name)

# STEP 2: Load audio manually and check properties
file = "voice.mp3"
waveform, sr = librosa.load(file, sr=16000)  # Required: 16kHz
print(f"Loaded audio: {file}")
print(f"Waveform shape: {waveform.shape}, Sample Rate: {sr}")

# Plot the waveform
plt.figure(figsize=(10, 2))
plt.plot(waveform)
plt.title("Audio Waveform")
plt.xlabel("Time (samples)")
plt.ylabel("Amplitude")
plt.tight_layout()
plt.show()

# STEP 3: Tokenize the audio (convert waveform to model input)
inputs = processor(waveform, sampling_rate=16000, return_tensors="pt", padding=True)

# View input values shape
print(f"Input shape to model: {inputs.input_values.shape}")  # [batch, time_steps]

# STEP 4: Run model inference
with torch.no_grad():
    logits = model(inputs.input_values).logits  # [batch, time_steps, vocab_size]

# View output logits
print(f"Logits shape: {logits.shape}")  # [1, time, vocab]

# STEP 5: Greedy decoding: take argmax at each time step
predicted_ids = torch.argmax(logits, dim=-1)
print("Predicted token IDs:", predicted_ids)

# STEP 6: Convert token IDs to string (transcription)
transcription = processor.batch_decode(predicted_ids)[0]
print("\nTranscription:", transcription)