# -*- coding: utf-8 -*-
"""spech-recognition

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WJeSXSGv1bly2ey1UXFm96SFOzGNmS9e
"""

# Display waveform and spectrogram of the audio file

import librosa
import librosa.display
import matplotlib.pyplot as plt

y, sr = librosa.load('voice.mp3')
plt.figure(figsize=(10, 4))
librosa.display.waveshow(y, sr=sr)
plt.title('Waveform')
plt.show()

# Spectrogram
X = librosa.stft(y)
Xdb = librosa.amplitude_to_db(abs(X))
plt.figure(figsize=(10, 4))
librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz')
plt.colorbar()
plt.title('Spectrogram')
plt.show()

# Compute Mel Spectrogram

import librosa
import librosa.display
import matplotlib.pyplot as plt
import numpy as np

# Load audio
y, sr = librosa.load('voice.mp3')

# Compute Mel Spectrogram
S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=8000)

# Convert to Decibel scale
S_dB = librosa.power_to_db(S, ref=np.max)

# Plot it
plt.figure(figsize=(12, 4))
librosa.display.specshow(S_dB, sr=sr, x_axis='time', y_axis='mel', fmax=8000)
plt.colorbar(format="%+2.0f dB")
plt.title('Mel Spectrogram (dB)')
plt.tight_layout()
plt.show()

import librosa
import librosa.display
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.nn.functional as F

# Load audio
y, sr = librosa.load('voice.mp3')

# Extract MFCC features
mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
mfccs = torch.tensor(mfccs.T, dtype=torch.float32).unsqueeze(0)  # [1, time, features]

labels = ["the", "of", "is", "play", "my", "but", "bet"]

# Traditional RNN model
class TraditionalRNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.rnn = nn.GRU(input_size=13, hidden_size=32, batch_first=True)
        self.fc = nn.Linear(32, len(labels))  # match the number of labels

    def forward(self, x):
        out, _ = self.rnn(x)
        return self.fc(out[:, -1, :])  # Use last time step

# Run model
model = TraditionalRNN()
output = model(mfccs)

# Convert logits to probabilities
probs = F.softmax(output, dim=1).detach().numpy().flatten()

# Find predicted label
predicted_idx = np.argmax(probs)
predicted_label = labels[predicted_idx]

# Print readable output
print("Probabilities:")
for i, label in enumerate(labels):
    print(f"{label:>10}: {probs[i]:.4f}")

print(f"\n Predicted label: **{predicted_label.upper()}**")

# Whisper: End-to-end speech recognition

from transformers import pipeline

# Load Whisper model
asr = pipeline("automatic-speech-recognition", model="openai/whisper-small")

# Run inference on audio
result = asr("voice.mp3")

# Print the transcription
print("Whisper Transcription:", result['text'])